I1203 01:54:03.302414 1969066752 caffe.cpp:177] Use CPU.
I1203 01:54:03.848923 1969066752 solver.cpp:47] Initializing solver from parameters: 
test_iter: 5
test_interval: 5
base_lr: 0.001
display: 1
max_iter: 300
lr_policy: "step"
gamma: 0.001
momentum: 0.9
weight_decay: 0.0005
stepsize: 15
snapshot: 20
snapshot_prefix: "weights"
solver_mode: CPU
net: "/Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt"
I1203 01:54:03.849221 1969066752 solver.cpp:90] Creating training net from net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt
I1203 01:54:03.849423 1969066752 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1203 01:54:03.849452 1969066752 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt"
    batch_size: 450
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reluConv"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "fc1"
  top: "relu1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh"
  type: "TanH"
  bottom: "fc2"
  top: "tanh"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "tanh"
  bottom: "labels"
  top: "loss"
}
I1203 01:54:03.849565 1969066752 layer_factory.hpp:76] Creating layer data
I1203 01:54:03.849578 1969066752 net.cpp:106] Creating Layer data
I1203 01:54:03.849583 1969066752 net.cpp:411] data -> data
I1203 01:54:03.849601 1969066752 net.cpp:411] data -> labels
I1203 01:54:03.849611 1969066752 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt
I1203 01:54:03.849639 1969066752 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I1203 01:54:03.850600 1969066752 hdf5.cpp:32] Datatype class: H5T_FLOAT
I1203 01:54:04.621397 1969066752 net.cpp:150] Setting up data
I1203 01:54:04.621423 1969066752 net.cpp:157] Top shape: 450 3 250 250 (84375000)
I1203 01:54:04.621433 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 01:54:04.621464 1969066752 net.cpp:165] Memory required for data: 337507200
I1203 01:54:04.621486 1969066752 layer_factory.hpp:76] Creating layer conv1
I1203 01:54:04.621500 1969066752 net.cpp:106] Creating Layer conv1
I1203 01:54:04.621508 1969066752 net.cpp:454] conv1 <- data
I1203 01:54:04.621515 1969066752 net.cpp:411] conv1 -> conv1
I1203 01:54:04.628695 1969066752 net.cpp:150] Setting up conv1
I1203 01:54:04.628706 1969066752 net.cpp:157] Top shape: 450 16 240 240 (414720000)
I1203 01:54:04.628715 1969066752 net.cpp:165] Memory required for data: 1996387200
I1203 01:54:04.628725 1969066752 layer_factory.hpp:76] Creating layer reluConv
I1203 01:54:04.628772 1969066752 net.cpp:106] Creating Layer reluConv
I1203 01:54:04.628779 1969066752 net.cpp:454] reluConv <- conv1
I1203 01:54:04.628787 1969066752 net.cpp:397] reluConv -> conv1 (in-place)
I1203 01:54:04.628796 1969066752 net.cpp:150] Setting up reluConv
I1203 01:54:04.628804 1969066752 net.cpp:157] Top shape: 450 16 240 240 (414720000)
I1203 01:54:04.628818 1969066752 net.cpp:165] Memory required for data: 3655267200
I1203 01:54:04.628825 1969066752 layer_factory.hpp:76] Creating layer conv2
I1203 01:54:04.628834 1969066752 net.cpp:106] Creating Layer conv2
I1203 01:54:04.628840 1969066752 net.cpp:454] conv2 <- conv1
I1203 01:54:04.628851 1969066752 net.cpp:411] conv2 -> conv2
I1203 01:54:04.629276 1969066752 net.cpp:150] Setting up conv2
I1203 01:54:04.629284 1969066752 net.cpp:157] Top shape: 450 16 115 115 (95220000)
I1203 01:54:04.629290 1969066752 net.cpp:165] Memory required for data: 4036147200
I1203 01:54:04.629298 1969066752 layer_factory.hpp:76] Creating layer fc1
I1203 01:54:04.629307 1969066752 net.cpp:106] Creating Layer fc1
I1203 01:54:04.629312 1969066752 net.cpp:454] fc1 <- conv2
I1203 01:54:04.629320 1969066752 net.cpp:411] fc1 -> fc1
I1203 01:54:04.696449 1969066752 net.cpp:150] Setting up fc1
I1203 01:54:04.696467 1969066752 net.cpp:157] Top shape: 450 40 (18000)
I1203 01:54:04.696483 1969066752 net.cpp:165] Memory required for data: 4036219200
I1203 01:54:04.696562 1969066752 layer_factory.hpp:76] Creating layer relu1
I1203 01:54:04.696576 1969066752 net.cpp:106] Creating Layer relu1
I1203 01:54:04.696583 1969066752 net.cpp:454] relu1 <- fc1
I1203 01:54:04.696593 1969066752 net.cpp:411] relu1 -> relu1
I1203 01:54:04.696604 1969066752 net.cpp:150] Setting up relu1
I1203 01:54:04.696607 1969066752 net.cpp:157] Top shape: 450 40 (18000)
I1203 01:54:04.696611 1969066752 net.cpp:165] Memory required for data: 4036291200
I1203 01:54:04.696615 1969066752 layer_factory.hpp:76] Creating layer fc2
I1203 01:54:04.696629 1969066752 net.cpp:106] Creating Layer fc2
I1203 01:54:04.696632 1969066752 net.cpp:454] fc2 <- relu1
I1203 01:54:04.696647 1969066752 net.cpp:411] fc2 -> fc2
I1203 01:54:04.696666 1969066752 net.cpp:150] Setting up fc2
I1203 01:54:04.696671 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 01:54:04.696683 1969066752 net.cpp:165] Memory required for data: 4036298400
I1203 01:54:04.696688 1969066752 layer_factory.hpp:76] Creating layer tanh
I1203 01:54:04.696704 1969066752 net.cpp:106] Creating Layer tanh
I1203 01:54:04.696712 1969066752 net.cpp:454] tanh <- fc2
I1203 01:54:04.696717 1969066752 net.cpp:411] tanh -> tanh
I1203 01:54:04.696724 1969066752 net.cpp:150] Setting up tanh
I1203 01:54:04.696728 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 01:54:04.696732 1969066752 net.cpp:165] Memory required for data: 4036305600
I1203 01:54:04.696737 1969066752 layer_factory.hpp:76] Creating layer loss
I1203 01:54:04.696743 1969066752 net.cpp:106] Creating Layer loss
I1203 01:54:04.696745 1969066752 net.cpp:454] loss <- tanh
I1203 01:54:04.696749 1969066752 net.cpp:454] loss <- labels
I1203 01:54:04.696764 1969066752 net.cpp:411] loss -> loss
I1203 01:54:04.696776 1969066752 net.cpp:150] Setting up loss
I1203 01:54:04.696780 1969066752 net.cpp:157] Top shape: (1)
I1203 01:54:04.696784 1969066752 net.cpp:160]     with loss weight 1
I1203 01:54:04.696807 1969066752 net.cpp:165] Memory required for data: 4036305604
I1203 01:54:04.696811 1969066752 net.cpp:226] loss needs backward computation.
I1203 01:54:04.696815 1969066752 net.cpp:226] tanh needs backward computation.
I1203 01:54:04.696818 1969066752 net.cpp:226] fc2 needs backward computation.
I1203 01:54:04.696821 1969066752 net.cpp:226] relu1 needs backward computation.
I1203 01:54:04.696825 1969066752 net.cpp:226] fc1 needs backward computation.
I1203 01:54:04.696830 1969066752 net.cpp:226] conv2 needs backward computation.
I1203 01:54:04.696832 1969066752 net.cpp:226] reluConv needs backward computation.
I1203 01:54:04.696836 1969066752 net.cpp:226] conv1 needs backward computation.
I1203 01:54:04.696841 1969066752 net.cpp:228] data does not need backward computation.
I1203 01:54:04.696871 1969066752 net.cpp:270] This network produces output loss
I1203 01:54:04.696879 1969066752 net.cpp:283] Network initialization done.
I1203 01:54:04.697131 1969066752 solver.cpp:180] Creating test net (#0) specified by net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt
I1203 01:54:04.697166 1969066752 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1203 01:54:04.697187 1969066752 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt"
    batch_size: 120
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reluConv"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "fc1"
  top: "relu1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh"
  type: "TanH"
  bottom: "fc2"
  top: "tanh"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "tanh"
  bottom: "labels"
  top: "loss"
}
I1203 01:54:04.697316 1969066752 layer_factory.hpp:76] Creating layer data
I1203 01:54:04.697324 1969066752 net.cpp:106] Creating Layer data
I1203 01:54:04.697329 1969066752 net.cpp:411] data -> data
I1203 01:54:04.697335 1969066752 net.cpp:411] data -> labels
I1203 01:54:04.697341 1969066752 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt
I1203 01:54:04.697365 1969066752 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I1203 01:54:04.928604 1969066752 net.cpp:150] Setting up data
I1203 01:54:04.928652 1969066752 net.cpp:157] Top shape: 120 3 250 250 (22500000)
I1203 01:54:04.928658 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 01:54:04.928663 1969066752 net.cpp:165] Memory required for data: 90001920
I1203 01:54:04.928671 1969066752 layer_factory.hpp:76] Creating layer conv1
I1203 01:54:04.928689 1969066752 net.cpp:106] Creating Layer conv1
I1203 01:54:04.928694 1969066752 net.cpp:454] conv1 <- data
I1203 01:54:04.928711 1969066752 net.cpp:411] conv1 -> conv1
I1203 01:54:04.929059 1969066752 net.cpp:150] Setting up conv1
I1203 01:54:04.929067 1969066752 net.cpp:157] Top shape: 120 16 240 240 (110592000)
I1203 01:54:04.929083 1969066752 net.cpp:165] Memory required for data: 532369920
I1203 01:54:04.929092 1969066752 layer_factory.hpp:76] Creating layer reluConv
I1203 01:54:04.929100 1969066752 net.cpp:106] Creating Layer reluConv
I1203 01:54:04.929103 1969066752 net.cpp:454] reluConv <- conv1
I1203 01:54:04.929119 1969066752 net.cpp:397] reluConv -> conv1 (in-place)
I1203 01:54:04.929185 1969066752 net.cpp:150] Setting up reluConv
I1203 01:54:04.929190 1969066752 net.cpp:157] Top shape: 120 16 240 240 (110592000)
I1203 01:54:04.929196 1969066752 net.cpp:165] Memory required for data: 974737920
I1203 01:54:04.929199 1969066752 layer_factory.hpp:76] Creating layer conv2
I1203 01:54:04.929206 1969066752 net.cpp:106] Creating Layer conv2
I1203 01:54:04.929210 1969066752 net.cpp:454] conv2 <- conv1
I1203 01:54:04.929215 1969066752 net.cpp:411] conv2 -> conv2
I1203 01:54:04.929579 1969066752 net.cpp:150] Setting up conv2
I1203 01:54:04.929584 1969066752 net.cpp:157] Top shape: 120 16 115 115 (25392000)
I1203 01:54:04.929600 1969066752 net.cpp:165] Memory required for data: 1076305920
I1203 01:54:04.929607 1969066752 layer_factory.hpp:76] Creating layer fc1
I1203 01:54:04.929615 1969066752 net.cpp:106] Creating Layer fc1
I1203 01:54:04.929620 1969066752 net.cpp:454] fc1 <- conv2
I1203 01:54:04.929638 1969066752 net.cpp:411] fc1 -> fc1
I1203 01:54:05.017209 1969066752 net.cpp:150] Setting up fc1
I1203 01:54:05.017235 1969066752 net.cpp:157] Top shape: 120 40 (4800)
I1203 01:54:05.017241 1969066752 net.cpp:165] Memory required for data: 1076325120
I1203 01:54:05.017251 1969066752 layer_factory.hpp:76] Creating layer relu1
I1203 01:54:05.017261 1969066752 net.cpp:106] Creating Layer relu1
I1203 01:54:05.017266 1969066752 net.cpp:454] relu1 <- fc1
I1203 01:54:05.017271 1969066752 net.cpp:411] relu1 -> relu1
I1203 01:54:05.017280 1969066752 net.cpp:150] Setting up relu1
I1203 01:54:05.017284 1969066752 net.cpp:157] Top shape: 120 40 (4800)
I1203 01:54:05.017288 1969066752 net.cpp:165] Memory required for data: 1076344320
I1203 01:54:05.017292 1969066752 layer_factory.hpp:76] Creating layer fc2
I1203 01:54:05.017299 1969066752 net.cpp:106] Creating Layer fc2
I1203 01:54:05.017304 1969066752 net.cpp:454] fc2 <- relu1
I1203 01:54:05.017309 1969066752 net.cpp:411] fc2 -> fc2
I1203 01:54:05.017325 1969066752 net.cpp:150] Setting up fc2
I1203 01:54:05.017330 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 01:54:05.017334 1969066752 net.cpp:165] Memory required for data: 1076346240
I1203 01:54:05.017339 1969066752 layer_factory.hpp:76] Creating layer tanh
I1203 01:54:05.017345 1969066752 net.cpp:106] Creating Layer tanh
I1203 01:54:05.017349 1969066752 net.cpp:454] tanh <- fc2
I1203 01:54:05.017354 1969066752 net.cpp:411] tanh -> tanh
I1203 01:54:05.017360 1969066752 net.cpp:150] Setting up tanh
I1203 01:54:05.017364 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 01:54:05.017369 1969066752 net.cpp:165] Memory required for data: 1076348160
I1203 01:54:05.017372 1969066752 layer_factory.hpp:76] Creating layer loss
I1203 01:54:05.017379 1969066752 net.cpp:106] Creating Layer loss
I1203 01:54:05.017382 1969066752 net.cpp:454] loss <- tanh
I1203 01:54:05.017386 1969066752 net.cpp:454] loss <- labels
I1203 01:54:05.017410 1969066752 net.cpp:411] loss -> loss
I1203 01:54:05.017428 1969066752 net.cpp:150] Setting up loss
I1203 01:54:05.017433 1969066752 net.cpp:157] Top shape: (1)
I1203 01:54:05.017438 1969066752 net.cpp:160]     with loss weight 1
I1203 01:54:05.017444 1969066752 net.cpp:165] Memory required for data: 1076348164
I1203 01:54:05.017448 1969066752 net.cpp:226] loss needs backward computation.
I1203 01:54:05.017452 1969066752 net.cpp:226] tanh needs backward computation.
I1203 01:54:05.017457 1969066752 net.cpp:226] fc2 needs backward computation.
I1203 01:54:05.017459 1969066752 net.cpp:226] relu1 needs backward computation.
I1203 01:54:05.017463 1969066752 net.cpp:226] fc1 needs backward computation.
I1203 01:54:05.017467 1969066752 net.cpp:226] conv2 needs backward computation.
I1203 01:54:05.017470 1969066752 net.cpp:226] reluConv needs backward computation.
I1203 01:54:05.017474 1969066752 net.cpp:226] conv1 needs backward computation.
I1203 01:54:05.017478 1969066752 net.cpp:228] data does not need backward computation.
I1203 01:54:05.017482 1969066752 net.cpp:270] This network produces output loss
I1203 01:54:05.017488 1969066752 net.cpp:283] Network initialization done.
I1203 01:54:05.017559 1969066752 solver.cpp:59] Solver scaffolding done.
I1203 01:54:05.017596 1969066752 caffe.cpp:212] Starting Optimization
I1203 01:54:05.017599 1969066752 solver.cpp:287] Solving net
I1203 01:54:05.017603 1969066752 solver.cpp:288] Learning Rate Policy: step
I1203 01:54:05.034865 1969066752 solver.cpp:340] Iteration 0, Testing net (#0)
I1203 01:54:46.061540 1969066752 solver.cpp:408]     Test net output #0: loss = 0.314923 (* 1 = 0.314923 loss)
I1203 01:56:10.868648 1969066752 solver.cpp:236] Iteration 0, loss = 0.322928
I1203 01:56:10.869398 1969066752 solver.cpp:252]     Train net output #0: loss = 0.322928 (* 1 = 0.322928 loss)
I1203 01:56:10.869652 1969066752 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1203 01:57:37.251318 1969066752 solver.cpp:236] Iteration 1, loss = 0.262006
I1203 01:57:37.252120 1969066752 solver.cpp:252]     Train net output #0: loss = 0.262006 (* 1 = 0.262006 loss)
I1203 01:57:37.252130 1969066752 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I1203 01:58:58.627132 1969066752 solver.cpp:236] Iteration 2, loss = 0.233961
I1203 01:58:58.627903 1969066752 solver.cpp:252]     Train net output #0: loss = 0.233961 (* 1 = 0.233961 loss)
I1203 01:58:58.627926 1969066752 sgd_solver.cpp:106] Iteration 2, lr = 0.001
I1203 02:00:24.899515 1969066752 solver.cpp:236] Iteration 3, loss = 0.219809
I1203 02:00:24.899845 1969066752 solver.cpp:252]     Train net output #0: loss = 0.219809 (* 1 = 0.219809 loss)
I1203 02:00:24.899865 1969066752 sgd_solver.cpp:106] Iteration 3, lr = 0.001
I1203 02:01:49.495332 1969066752 solver.cpp:236] Iteration 4, loss = 0.207858
I1203 02:01:49.495375 1969066752 solver.cpp:252]     Train net output #0: loss = 0.207858 (* 1 = 0.207858 loss)
I1203 02:01:49.495383 1969066752 sgd_solver.cpp:106] Iteration 4, lr = 0.001
I1203 02:01:49.517709 1969066752 solver.cpp:340] Iteration 5, Testing net (#0)
I1203 02:02:32.629358 1969066752 solver.cpp:408]     Test net output #0: loss = 0.183988 (* 1 = 0.183988 loss)
I1203 02:03:55.300681 1969066752 solver.cpp:236] Iteration 5, loss = 0.186603
I1203 02:03:55.300722 1969066752 solver.cpp:252]     Train net output #0: loss = 0.186603 (* 1 = 0.186603 loss)
I1203 02:03:55.300730 1969066752 sgd_solver.cpp:106] Iteration 5, lr = 0.001
I1203 02:05:16.395264 1969066752 solver.cpp:236] Iteration 6, loss = 0.185591
I1203 02:05:16.395304 1969066752 solver.cpp:252]     Train net output #0: loss = 0.185591 (* 1 = 0.185591 loss)
I1203 02:05:16.395311 1969066752 sgd_solver.cpp:106] Iteration 6, lr = 0.001
I1203 02:06:37.301030 1969066752 solver.cpp:236] Iteration 7, loss = 0.177589
I1203 02:06:37.301074 1969066752 solver.cpp:252]     Train net output #0: loss = 0.177589 (* 1 = 0.177589 loss)
I1203 02:06:37.301080 1969066752 sgd_solver.cpp:106] Iteration 7, lr = 0.001
I1203 02:07:58.377504 1969066752 solver.cpp:236] Iteration 8, loss = 0.180787
I1203 02:07:58.377550 1969066752 solver.cpp:252]     Train net output #0: loss = 0.180787 (* 1 = 0.180787 loss)
I1203 02:07:58.377557 1969066752 sgd_solver.cpp:106] Iteration 8, lr = 0.001
I1203 02:09:19.268218 1969066752 solver.cpp:236] Iteration 9, loss = 0.169743
I1203 02:09:19.268262 1969066752 solver.cpp:252]     Train net output #0: loss = 0.169743 (* 1 = 0.169743 loss)
I1203 02:09:19.268270 1969066752 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I1203 02:09:19.290225 1969066752 solver.cpp:340] Iteration 10, Testing net (#0)
I1203 02:10:00.072343 1969066752 solver.cpp:408]     Test net output #0: loss = 0.159662 (* 1 = 0.159662 loss)
I1203 02:11:29.090036 1969066752 solver.cpp:236] Iteration 10, loss = 0.166125
I1203 02:11:29.090848 1969066752 solver.cpp:252]     Train net output #0: loss = 0.166125 (* 1 = 0.166125 loss)
I1203 02:11:29.090860 1969066752 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1203 02:12:51.155498 1969066752 solver.cpp:236] Iteration 11, loss = 0.159349
I1203 02:12:51.155558 1969066752 solver.cpp:252]     Train net output #0: loss = 0.159349 (* 1 = 0.159349 loss)
I1203 02:12:51.155566 1969066752 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1203 02:14:12.125298 1969066752 solver.cpp:236] Iteration 12, loss = 0.157199
I1203 02:14:12.125341 1969066752 solver.cpp:252]     Train net output #0: loss = 0.157199 (* 1 = 0.157199 loss)
I1203 02:14:12.125349 1969066752 sgd_solver.cpp:106] Iteration 12, lr = 0.001
I1203 02:15:34.780488 1969066752 solver.cpp:236] Iteration 13, loss = 0.151647
I1203 02:15:34.780534 1969066752 solver.cpp:252]     Train net output #0: loss = 0.151647 (* 1 = 0.151647 loss)
I1203 02:15:34.780540 1969066752 sgd_solver.cpp:106] Iteration 13, lr = 0.001
I1203 02:16:55.778168 1969066752 solver.cpp:236] Iteration 14, loss = 0.149501
I1203 02:16:55.778213 1969066752 solver.cpp:252]     Train net output #0: loss = 0.149501 (* 1 = 0.149501 loss)
I1203 02:16:55.778220 1969066752 sgd_solver.cpp:106] Iteration 14, lr = 0.001
I1203 02:16:55.798570 1969066752 solver.cpp:340] Iteration 15, Testing net (#0)
I1203 02:17:35.006697 1969066752 solver.cpp:408]     Test net output #0: loss = 0.142403 (* 1 = 0.142403 loss)
I1203 02:18:56.459827 1969066752 solver.cpp:236] Iteration 15, loss = 0.14617
I1203 02:18:56.459870 1969066752 solver.cpp:252]     Train net output #0: loss = 0.14617 (* 1 = 0.14617 loss)
I1203 02:18:56.459877 1969066752 sgd_solver.cpp:106] Iteration 15, lr = 1e-06
I1203 02:20:17.397101 1969066752 solver.cpp:236] Iteration 16, loss = 0.140283
I1203 02:20:17.397146 1969066752 solver.cpp:252]     Train net output #0: loss = 0.140283 (* 1 = 0.140283 loss)
I1203 02:20:17.397155 1969066752 sgd_solver.cpp:106] Iteration 16, lr = 1e-06
I1203 02:21:39.245529 1969066752 solver.cpp:236] Iteration 17, loss = 0.144432
I1203 02:21:39.245573 1969066752 solver.cpp:252]     Train net output #0: loss = 0.144432 (* 1 = 0.144432 loss)
I1203 02:21:39.245579 1969066752 sgd_solver.cpp:106] Iteration 17, lr = 1e-06
I1203 02:23:00.445847 1969066752 solver.cpp:236] Iteration 18, loss = 0.138716
I1203 02:23:00.445890 1969066752 solver.cpp:252]     Train net output #0: loss = 0.138716 (* 1 = 0.138716 loss)
I1203 02:23:00.445897 1969066752 sgd_solver.cpp:106] Iteration 18, lr = 1e-06
I1203 02:24:21.476475 1969066752 solver.cpp:236] Iteration 19, loss = 0.142194
I1203 02:24:21.476517 1969066752 solver.cpp:252]     Train net output #0: loss = 0.142194 (* 1 = 0.142194 loss)
I1203 02:24:21.476526 1969066752 sgd_solver.cpp:106] Iteration 19, lr = 1e-06
I1203 02:24:21.496881 1969066752 solver.cpp:461] Snapshotting to binary proto file weights_iter_20.caffemodel
I1203 02:24:21.739904 1969066752 sgd_solver.cpp:269] Snapshotting solver state to binary proto file weights_iter_20.solverstate
I1203 02:24:21.943316 1969066752 solver.cpp:340] Iteration 20, Testing net (#0)
I1203 02:25:01.082890 1969066752 solver.cpp:408]     Test net output #0: loss = 0.13775 (* 1 = 0.13775 loss)
I1203 02:26:23.137059 1969066752 solver.cpp:236] Iteration 20, loss = 0.137176
I1203 02:26:23.137104 1969066752 solver.cpp:252]     Train net output #0: loss = 0.137176 (* 1 = 0.137176 loss)
I1203 02:26:23.137110 1969066752 sgd_solver.cpp:106] Iteration 20, lr = 1e-06
I1203 02:27:44.239585 1969066752 solver.cpp:236] Iteration 21, loss = 0.146493
I1203 02:27:44.239629 1969066752 solver.cpp:252]     Train net output #0: loss = 0.146493 (* 1 = 0.146493 loss)
I1203 02:27:44.239636 1969066752 sgd_solver.cpp:106] Iteration 21, lr = 1e-06
I1203 02:29:05.092491 1969066752 solver.cpp:236] Iteration 22, loss = 0.137888
I1203 02:29:05.092535 1969066752 solver.cpp:252]     Train net output #0: loss = 0.137888 (* 1 = 0.137888 loss)
I1203 02:29:05.092542 1969066752 sgd_solver.cpp:106] Iteration 22, lr = 1e-06
I1203 02:30:26.052727 1969066752 solver.cpp:236] Iteration 23, loss = 0.148321
I1203 02:30:26.052772 1969066752 solver.cpp:252]     Train net output #0: loss = 0.148321 (* 1 = 0.148321 loss)
I1203 02:30:26.052779 1969066752 sgd_solver.cpp:106] Iteration 23, lr = 1e-06
I1203 02:31:47.685309 1969066752 solver.cpp:236] Iteration 24, loss = 0.13935
I1203 02:31:47.685369 1969066752 solver.cpp:252]     Train net output #0: loss = 0.13935 (* 1 = 0.13935 loss)
I1203 02:31:47.685377 1969066752 sgd_solver.cpp:106] Iteration 24, lr = 1e-06
I1203 02:31:47.706429 1969066752 solver.cpp:340] Iteration 25, Testing net (#0)
I1203 02:32:26.982838 1969066752 solver.cpp:408]     Test net output #0: loss = 0.141007 (* 1 = 0.141007 loss)
I1203 02:33:48.209362 1969066752 solver.cpp:236] Iteration 25, loss = 0.147397
I1203 02:33:48.209404 1969066752 solver.cpp:252]     Train net output #0: loss = 0.147397 (* 1 = 0.147397 loss)
I1203 02:33:48.209411 1969066752 sgd_solver.cpp:106] Iteration 25, lr = 1e-06
I1203 02:35:09.180260 1969066752 solver.cpp:236] Iteration 26, loss = 0.148584
I1203 02:35:09.180382 1969066752 solver.cpp:252]     Train net output #0: loss = 0.148584 (* 1 = 0.148584 loss)
I1203 02:35:09.180390 1969066752 sgd_solver.cpp:106] Iteration 26, lr = 1e-06
I1203 02:36:30.811977 1969066752 solver.cpp:236] Iteration 27, loss = 0.144756
I1203 02:36:30.812019 1969066752 solver.cpp:252]     Train net output #0: loss = 0.144756 (* 1 = 0.144756 loss)
I1203 02:36:30.812027 1969066752 sgd_solver.cpp:106] Iteration 27, lr = 1e-06
