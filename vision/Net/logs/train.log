I1203 17:27:47.972681 1969066752 caffe.cpp:177] Use CPU.
I1203 17:27:48.647493 1969066752 solver.cpp:47] Initializing solver from parameters: 
test_iter: 1
test_interval: 1
base_lr: 0.001
display: 1
max_iter: 300
lr_policy: "step"
gamma: 1e-05
momentum: 0.9
weight_decay: 0.0005
stepsize: 20
snapshot: 20
snapshot_prefix: "weights"
solver_mode: CPU
net: "/Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt"
I1203 17:27:48.647797 1969066752 solver.cpp:90] Creating training net from net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt
I1203 17:27:48.648037 1969066752 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1203 17:27:48.648064 1969066752 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt"
    batch_size: 450
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reluConv"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh1"
  type: "TanH"
  bottom: "fc1"
  top: "tanh1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "tanh1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh"
  type: "TanH"
  bottom: "fc2"
  top: "tanh"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "tanh"
  bottom: "labels"
  top: "loss"
}
I1203 17:27:48.648229 1969066752 layer_factory.hpp:76] Creating layer data
I1203 17:27:48.648242 1969066752 net.cpp:106] Creating Layer data
I1203 17:27:48.648247 1969066752 net.cpp:411] data -> data
I1203 17:27:48.648264 1969066752 net.cpp:411] data -> labels
I1203 17:27:48.648279 1969066752 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt
I1203 17:27:48.648314 1969066752 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I1203 17:27:48.670055 1969066752 hdf5.cpp:32] Datatype class: H5T_FLOAT
I1203 17:27:49.457767 1969066752 net.cpp:150] Setting up data
I1203 17:27:49.457793 1969066752 net.cpp:157] Top shape: 450 3 250 250 (84375000)
I1203 17:27:49.457803 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 17:27:49.457808 1969066752 net.cpp:165] Memory required for data: 337507200
I1203 17:27:49.457815 1969066752 layer_factory.hpp:76] Creating layer conv1
I1203 17:27:49.457825 1969066752 net.cpp:106] Creating Layer conv1
I1203 17:27:49.457831 1969066752 net.cpp:454] conv1 <- data
I1203 17:27:49.457839 1969066752 net.cpp:411] conv1 -> conv1
I1203 17:27:49.462069 1969066752 net.cpp:150] Setting up conv1
I1203 17:27:49.462077 1969066752 net.cpp:157] Top shape: 450 16 240 240 (414720000)
I1203 17:27:49.462093 1969066752 net.cpp:165] Memory required for data: 1996387200
I1203 17:27:49.462142 1969066752 layer_factory.hpp:76] Creating layer reluConv
I1203 17:27:49.462194 1969066752 net.cpp:106] Creating Layer reluConv
I1203 17:27:49.462201 1969066752 net.cpp:454] reluConv <- conv1
I1203 17:27:49.462239 1969066752 net.cpp:397] reluConv -> conv1 (in-place)
I1203 17:27:49.462254 1969066752 net.cpp:150] Setting up reluConv
I1203 17:27:49.462267 1969066752 net.cpp:157] Top shape: 450 16 240 240 (414720000)
I1203 17:27:49.462272 1969066752 net.cpp:165] Memory required for data: 3655267200
I1203 17:27:49.462287 1969066752 layer_factory.hpp:76] Creating layer conv2
I1203 17:27:49.462296 1969066752 net.cpp:106] Creating Layer conv2
I1203 17:27:49.462298 1969066752 net.cpp:454] conv2 <- conv1
I1203 17:27:49.462304 1969066752 net.cpp:411] conv2 -> conv2
I1203 17:27:49.462687 1969066752 net.cpp:150] Setting up conv2
I1203 17:27:49.462697 1969066752 net.cpp:157] Top shape: 450 16 115 115 (95220000)
I1203 17:27:49.462714 1969066752 net.cpp:165] Memory required for data: 4036147200
I1203 17:27:49.462721 1969066752 layer_factory.hpp:76] Creating layer fc1
I1203 17:27:49.462741 1969066752 net.cpp:106] Creating Layer fc1
I1203 17:27:49.462745 1969066752 net.cpp:454] fc1 <- conv2
I1203 17:27:49.462751 1969066752 net.cpp:411] fc1 -> fc1
I1203 17:27:49.530269 1969066752 net.cpp:150] Setting up fc1
I1203 17:27:49.530293 1969066752 net.cpp:157] Top shape: 450 40 (18000)
I1203 17:27:49.530309 1969066752 net.cpp:165] Memory required for data: 4036219200
I1203 17:27:49.530319 1969066752 layer_factory.hpp:76] Creating layer tanh1
I1203 17:27:49.530329 1969066752 net.cpp:106] Creating Layer tanh1
I1203 17:27:49.530333 1969066752 net.cpp:454] tanh1 <- fc1
I1203 17:27:49.530339 1969066752 net.cpp:411] tanh1 -> tanh1
I1203 17:27:49.530359 1969066752 net.cpp:150] Setting up tanh1
I1203 17:27:49.530362 1969066752 net.cpp:157] Top shape: 450 40 (18000)
I1203 17:27:49.530366 1969066752 net.cpp:165] Memory required for data: 4036291200
I1203 17:27:49.530369 1969066752 layer_factory.hpp:76] Creating layer fc2
I1203 17:27:49.530385 1969066752 net.cpp:106] Creating Layer fc2
I1203 17:27:49.530400 1969066752 net.cpp:454] fc2 <- tanh1
I1203 17:27:49.530405 1969066752 net.cpp:411] fc2 -> fc2
I1203 17:27:49.530431 1969066752 net.cpp:150] Setting up fc2
I1203 17:27:49.530434 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 17:27:49.530441 1969066752 net.cpp:165] Memory required for data: 4036298400
I1203 17:27:49.530450 1969066752 layer_factory.hpp:76] Creating layer tanh
I1203 17:27:49.530455 1969066752 net.cpp:106] Creating Layer tanh
I1203 17:27:49.530459 1969066752 net.cpp:454] tanh <- fc2
I1203 17:27:49.530467 1969066752 net.cpp:411] tanh -> tanh
I1203 17:27:49.530472 1969066752 net.cpp:150] Setting up tanh
I1203 17:27:49.530477 1969066752 net.cpp:157] Top shape: 450 4 (1800)
I1203 17:27:49.530480 1969066752 net.cpp:165] Memory required for data: 4036305600
I1203 17:27:49.530483 1969066752 layer_factory.hpp:76] Creating layer loss
I1203 17:27:49.530490 1969066752 net.cpp:106] Creating Layer loss
I1203 17:27:49.530494 1969066752 net.cpp:454] loss <- tanh
I1203 17:27:49.530498 1969066752 net.cpp:454] loss <- labels
I1203 17:27:49.530503 1969066752 net.cpp:411] loss -> loss
I1203 17:27:49.530517 1969066752 net.cpp:150] Setting up loss
I1203 17:27:49.530521 1969066752 net.cpp:157] Top shape: (1)
I1203 17:27:49.530534 1969066752 net.cpp:160]     with loss weight 1
I1203 17:27:49.530558 1969066752 net.cpp:165] Memory required for data: 4036305604
I1203 17:27:49.530561 1969066752 net.cpp:226] loss needs backward computation.
I1203 17:27:49.530565 1969066752 net.cpp:226] tanh needs backward computation.
I1203 17:27:49.530570 1969066752 net.cpp:226] fc2 needs backward computation.
I1203 17:27:49.530572 1969066752 net.cpp:226] tanh1 needs backward computation.
I1203 17:27:49.530575 1969066752 net.cpp:226] fc1 needs backward computation.
I1203 17:27:49.530580 1969066752 net.cpp:226] conv2 needs backward computation.
I1203 17:27:49.530582 1969066752 net.cpp:226] reluConv needs backward computation.
I1203 17:27:49.530586 1969066752 net.cpp:226] conv1 needs backward computation.
I1203 17:27:49.530604 1969066752 net.cpp:228] data does not need backward computation.
I1203 17:27:49.530637 1969066752 net.cpp:270] This network produces output loss
I1203 17:27:49.530645 1969066752 net.cpp:283] Network initialization done.
I1203 17:27:49.530899 1969066752 solver.cpp:180] Creating test net (#0) specified by net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/trainer.prototxt
I1203 17:27:49.530930 1969066752 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I1203 17:27:49.530949 1969066752 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt"
    batch_size: 120
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reluConv"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh1"
  type: "TanH"
  bottom: "fc1"
  top: "tanh1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "tanh1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanh"
  type: "TanH"
  bottom: "fc2"
  top: "tanh"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "tanh"
  bottom: "labels"
  top: "loss"
}
I1203 17:27:49.531095 1969066752 layer_factory.hpp:76] Creating layer data
I1203 17:27:49.531101 1969066752 net.cpp:106] Creating Layer data
I1203 17:27:49.531106 1969066752 net.cpp:411] data -> data
I1203 17:27:49.531114 1969066752 net.cpp:411] data -> labels
I1203 17:27:49.531121 1969066752 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt
I1203 17:27:49.531152 1969066752 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I1203 17:27:49.753582 1969066752 net.cpp:150] Setting up data
I1203 17:27:49.753604 1969066752 net.cpp:157] Top shape: 120 3 250 250 (22500000)
I1203 17:27:49.753612 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 17:27:49.753615 1969066752 net.cpp:165] Memory required for data: 90001920
I1203 17:27:49.753621 1969066752 layer_factory.hpp:76] Creating layer conv1
I1203 17:27:49.753633 1969066752 net.cpp:106] Creating Layer conv1
I1203 17:27:49.753638 1969066752 net.cpp:454] conv1 <- data
I1203 17:27:49.753644 1969066752 net.cpp:411] conv1 -> conv1
I1203 17:27:49.753867 1969066752 net.cpp:150] Setting up conv1
I1203 17:27:49.753878 1969066752 net.cpp:157] Top shape: 120 16 240 240 (110592000)
I1203 17:27:49.753885 1969066752 net.cpp:165] Memory required for data: 532369920
I1203 17:27:49.753891 1969066752 layer_factory.hpp:76] Creating layer reluConv
I1203 17:27:49.753898 1969066752 net.cpp:106] Creating Layer reluConv
I1203 17:27:49.753902 1969066752 net.cpp:454] reluConv <- conv1
I1203 17:27:49.753907 1969066752 net.cpp:397] reluConv -> conv1 (in-place)
I1203 17:27:49.753938 1969066752 net.cpp:150] Setting up reluConv
I1203 17:27:49.753942 1969066752 net.cpp:157] Top shape: 120 16 240 240 (110592000)
I1203 17:27:49.753947 1969066752 net.cpp:165] Memory required for data: 974737920
I1203 17:27:49.753952 1969066752 layer_factory.hpp:76] Creating layer conv2
I1203 17:27:49.753957 1969066752 net.cpp:106] Creating Layer conv2
I1203 17:27:49.753962 1969066752 net.cpp:454] conv2 <- conv1
I1203 17:27:49.753967 1969066752 net.cpp:411] conv2 -> conv2
I1203 17:27:49.754309 1969066752 net.cpp:150] Setting up conv2
I1203 17:27:49.754314 1969066752 net.cpp:157] Top shape: 120 16 115 115 (25392000)
I1203 17:27:49.754330 1969066752 net.cpp:165] Memory required for data: 1076305920
I1203 17:27:49.754336 1969066752 layer_factory.hpp:76] Creating layer fc1
I1203 17:27:49.754343 1969066752 net.cpp:106] Creating Layer fc1
I1203 17:27:49.754346 1969066752 net.cpp:454] fc1 <- conv2
I1203 17:27:49.754364 1969066752 net.cpp:411] fc1 -> fc1
I1203 17:27:49.821388 1969066752 net.cpp:150] Setting up fc1
I1203 17:27:49.821405 1969066752 net.cpp:157] Top shape: 120 40 (4800)
I1203 17:27:49.821421 1969066752 net.cpp:165] Memory required for data: 1076325120
I1203 17:27:49.821497 1969066752 layer_factory.hpp:76] Creating layer tanh1
I1203 17:27:49.821512 1969066752 net.cpp:106] Creating Layer tanh1
I1203 17:27:49.821517 1969066752 net.cpp:454] tanh1 <- fc1
I1203 17:27:49.821524 1969066752 net.cpp:411] tanh1 -> tanh1
I1203 17:27:49.821533 1969066752 net.cpp:150] Setting up tanh1
I1203 17:27:49.821537 1969066752 net.cpp:157] Top shape: 120 40 (4800)
I1203 17:27:49.821542 1969066752 net.cpp:165] Memory required for data: 1076344320
I1203 17:27:49.821545 1969066752 layer_factory.hpp:76] Creating layer fc2
I1203 17:27:49.821552 1969066752 net.cpp:106] Creating Layer fc2
I1203 17:27:49.821560 1969066752 net.cpp:454] fc2 <- tanh1
I1203 17:27:49.821566 1969066752 net.cpp:411] fc2 -> fc2
I1203 17:27:49.821584 1969066752 net.cpp:150] Setting up fc2
I1203 17:27:49.821588 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 17:27:49.821601 1969066752 net.cpp:165] Memory required for data: 1076346240
I1203 17:27:49.821606 1969066752 layer_factory.hpp:76] Creating layer tanh
I1203 17:27:49.821621 1969066752 net.cpp:106] Creating Layer tanh
I1203 17:27:49.821624 1969066752 net.cpp:454] tanh <- fc2
I1203 17:27:49.821630 1969066752 net.cpp:411] tanh -> tanh
I1203 17:27:49.821635 1969066752 net.cpp:150] Setting up tanh
I1203 17:27:49.821638 1969066752 net.cpp:157] Top shape: 120 4 (480)
I1203 17:27:49.821642 1969066752 net.cpp:165] Memory required for data: 1076348160
I1203 17:27:49.821646 1969066752 layer_factory.hpp:76] Creating layer loss
I1203 17:27:49.821652 1969066752 net.cpp:106] Creating Layer loss
I1203 17:27:49.821657 1969066752 net.cpp:454] loss <- tanh
I1203 17:27:49.821661 1969066752 net.cpp:454] loss <- labels
I1203 17:27:49.821666 1969066752 net.cpp:411] loss -> loss
I1203 17:27:49.821674 1969066752 net.cpp:150] Setting up loss
I1203 17:27:49.821678 1969066752 net.cpp:157] Top shape: (1)
I1203 17:27:49.821691 1969066752 net.cpp:160]     with loss weight 1
I1203 17:27:49.821698 1969066752 net.cpp:165] Memory required for data: 1076348164
I1203 17:27:49.821702 1969066752 net.cpp:226] loss needs backward computation.
I1203 17:27:49.821705 1969066752 net.cpp:226] tanh needs backward computation.
I1203 17:27:49.821708 1969066752 net.cpp:226] fc2 needs backward computation.
I1203 17:27:49.821712 1969066752 net.cpp:226] tanh1 needs backward computation.
I1203 17:27:49.821715 1969066752 net.cpp:226] fc1 needs backward computation.
I1203 17:27:49.821718 1969066752 net.cpp:226] conv2 needs backward computation.
I1203 17:27:49.821732 1969066752 net.cpp:226] reluConv needs backward computation.
I1203 17:27:49.821735 1969066752 net.cpp:226] conv1 needs backward computation.
I1203 17:27:49.821739 1969066752 net.cpp:228] data does not need backward computation.
I1203 17:27:49.821743 1969066752 net.cpp:270] This network produces output loss
I1203 17:27:49.821748 1969066752 net.cpp:283] Network initialization done.
I1203 17:27:49.821827 1969066752 solver.cpp:59] Solver scaffolding done.
I1203 17:27:49.821867 1969066752 caffe.cpp:212] Starting Optimization
I1203 17:27:49.821872 1969066752 solver.cpp:287] Solving net
I1203 17:27:49.821876 1969066752 solver.cpp:288] Learning Rate Policy: step
I1203 17:27:49.835942 1969066752 solver.cpp:340] Iteration 0, Testing net (#0)
I1203 17:27:58.082787 1969066752 solver.cpp:408]     Test net output #0: loss = 0.437659 (* 1 = 0.437659 loss)
I1203 17:29:22.118474 1969066752 solver.cpp:236] Iteration 0, loss = 0.426946
I1203 17:29:22.119266 1969066752 solver.cpp:252]     Train net output #0: loss = 0.426946 (* 1 = 0.426946 loss)
I1203 17:29:22.119515 1969066752 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1203 17:29:22.164842 1969066752 solver.cpp:340] Iteration 1, Testing net (#0)
I1203 17:29:30.893714 1969066752 solver.cpp:408]     Test net output #0: loss = 0.690883 (* 1 = 0.690883 loss)
I1203 17:30:53.163987 1969066752 solver.cpp:236] Iteration 1, loss = 0.75353
I1203 17:30:53.164788 1969066752 solver.cpp:252]     Train net output #0: loss = 0.75353 (* 1 = 0.75353 loss)
I1203 17:30:53.164798 1969066752 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I1203 17:30:53.187376 1969066752 solver.cpp:340] Iteration 2, Testing net (#0)
I1203 17:31:01.085389 1969066752 solver.cpp:408]     Test net output #0: loss = 0.722492 (* 1 = 0.722492 loss)
I1203 17:32:22.772689 1969066752 solver.cpp:236] Iteration 2, loss = 0.752022
I1203 17:32:22.773491 1969066752 solver.cpp:252]     Train net output #0: loss = 0.752022 (* 1 = 0.752022 loss)
I1203 17:32:22.773520 1969066752 sgd_solver.cpp:106] Iteration 2, lr = 0.001
I1203 17:32:22.796720 1969066752 solver.cpp:340] Iteration 3, Testing net (#0)
I1203 17:32:30.625566 1969066752 solver.cpp:408]     Test net output #0: loss = 0.612701 (* 1 = 0.612701 loss)
I1203 17:33:51.543957 1969066752 solver.cpp:236] Iteration 3, loss = 0.63775
I1203 17:33:51.543999 1969066752 solver.cpp:252]     Train net output #0: loss = 0.63775 (* 1 = 0.63775 loss)
I1203 17:33:51.544006 1969066752 sgd_solver.cpp:106] Iteration 3, lr = 0.001
I1203 17:33:51.564934 1969066752 solver.cpp:340] Iteration 4, Testing net (#0)
I1203 17:33:59.399862 1969066752 solver.cpp:408]     Test net output #0: loss = 0.628398 (* 1 = 0.628398 loss)
I1203 17:35:20.472802 1969066752 solver.cpp:236] Iteration 4, loss = 0.606611
I1203 17:35:20.472844 1969066752 solver.cpp:252]     Train net output #0: loss = 0.606611 (* 1 = 0.606611 loss)
I1203 17:35:20.472852 1969066752 sgd_solver.cpp:106] Iteration 4, lr = 0.001
I1203 17:35:20.494601 1969066752 solver.cpp:340] Iteration 5, Testing net (#0)
I1203 17:35:28.323453 1969066752 solver.cpp:408]     Test net output #0: loss = 0.522035 (* 1 = 0.522035 loss)
I1203 17:36:49.287813 1969066752 solver.cpp:236] Iteration 5, loss = 0.579274
I1203 17:36:49.287855 1969066752 solver.cpp:252]     Train net output #0: loss = 0.579274 (* 1 = 0.579274 loss)
I1203 17:36:49.287863 1969066752 sgd_solver.cpp:106] Iteration 5, lr = 0.001
I1203 17:36:49.309535 1969066752 solver.cpp:340] Iteration 6, Testing net (#0)
I1203 17:36:57.141521 1969066752 solver.cpp:408]     Test net output #0: loss = 0.477953 (* 1 = 0.477953 loss)
I1203 17:38:18.343598 1969066752 solver.cpp:236] Iteration 6, loss = 0.471169
I1203 17:38:18.343641 1969066752 solver.cpp:252]     Train net output #0: loss = 0.471169 (* 1 = 0.471169 loss)
I1203 17:38:18.343647 1969066752 sgd_solver.cpp:106] Iteration 6, lr = 0.001
I1203 17:38:18.364245 1969066752 solver.cpp:340] Iteration 7, Testing net (#0)
I1203 17:38:26.263439 1969066752 solver.cpp:408]     Test net output #0: loss = 0.265401 (* 1 = 0.265401 loss)
I1203 17:39:47.227813 1969066752 solver.cpp:236] Iteration 7, loss = 0.25409
I1203 17:39:47.227855 1969066752 solver.cpp:252]     Train net output #0: loss = 0.25409 (* 1 = 0.25409 loss)
I1203 17:39:47.227862 1969066752 sgd_solver.cpp:106] Iteration 7, lr = 0.001
I1203 17:39:47.248749 1969066752 solver.cpp:340] Iteration 8, Testing net (#0)
I1203 17:39:55.060509 1969066752 solver.cpp:408]     Test net output #0: loss = 0.407061 (* 1 = 0.407061 loss)
I1203 17:41:15.891186 1969066752 solver.cpp:236] Iteration 8, loss = 0.416744
I1203 17:41:15.891935 1969066752 solver.cpp:252]     Train net output #0: loss = 0.416744 (* 1 = 0.416744 loss)
I1203 17:41:15.891943 1969066752 sgd_solver.cpp:106] Iteration 8, lr = 0.001
I1203 17:41:15.912719 1969066752 solver.cpp:340] Iteration 9, Testing net (#0)
I1203 17:41:23.821336 1969066752 solver.cpp:408]     Test net output #0: loss = 0.562875 (* 1 = 0.562875 loss)
I1203 17:42:44.696882 1969066752 solver.cpp:236] Iteration 9, loss = 0.570626
I1203 17:42:44.696926 1969066752 solver.cpp:252]     Train net output #0: loss = 0.570626 (* 1 = 0.570626 loss)
I1203 17:42:44.696933 1969066752 sgd_solver.cpp:106] Iteration 9, lr = 0.001
I1203 17:42:44.718382 1969066752 solver.cpp:340] Iteration 10, Testing net (#0)
I1203 17:42:52.554898 1969066752 solver.cpp:408]     Test net output #0: loss = 0.521066 (* 1 = 0.521066 loss)
I1203 17:46:26.851614 1969066752 solver.cpp:236] Iteration 10, loss = 0.588839
I1203 17:46:26.851657 1969066752 solver.cpp:252]     Train net output #0: loss = 0.588839 (* 1 = 0.588839 loss)
I1203 17:46:26.851665 1969066752 sgd_solver.cpp:106] Iteration 10, lr = 0.001
I1203 17:46:26.873726 1969066752 solver.cpp:340] Iteration 11, Testing net (#0)
I1203 17:46:34.782228 1969066752 solver.cpp:408]     Test net output #0: loss = 0.60036 (* 1 = 0.60036 loss)
I1203 17:47:56.720085 1969066752 solver.cpp:236] Iteration 11, loss = 0.544132
I1203 17:47:56.720127 1969066752 solver.cpp:252]     Train net output #0: loss = 0.544132 (* 1 = 0.544132 loss)
I1203 17:47:56.720134 1969066752 sgd_solver.cpp:106] Iteration 11, lr = 0.001
I1203 17:47:56.743191 1969066752 solver.cpp:340] Iteration 12, Testing net (#0)
I1203 17:48:04.922484 1969066752 solver.cpp:408]     Test net output #0: loss = 0.508803 (* 1 = 0.508803 loss)
