I0110 22:50:09.492156 2090763008 caffe.cpp:177] Use CPU.
I0110 22:50:10.190431 2090763008 solver.cpp:47] Initializing solver from parameters: 
test_iter: 1
test_interval: 1
base_lr: 0.001
display: 1
max_iter: 300
lr_policy: "step"
gamma: 1e-05
momentum: 0.9
weight_decay: 0.0005
stepsize: 10
snapshot: 5
snapshot_prefix: "weights"
solver_mode: CPU
net: "/Users/JonathanLee/Desktop/sandbox/vision/Net/nets/net8/trainer8.prototxt"
I0110 22:50:10.190733 2090763008 solver.cpp:90] Creating training net from net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/nets/net8/trainer8.prototxt
I0110 22:50:10.190986 2090763008 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0110 22:50:10.191011 2090763008 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt"
    batch_size: 450
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanhConv1"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanhConv2"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out"
  type: "TanH"
  bottom: "fc1"
  top: "out"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "out"
  bottom: "labels"
  top: "loss"
}
I0110 22:50:10.191165 2090763008 layer_factory.hpp:76] Creating layer data
I0110 22:50:10.191177 2090763008 net.cpp:106] Creating Layer data
I0110 22:50:10.191184 2090763008 net.cpp:411] data -> data
I0110 22:50:10.191200 2090763008 net.cpp:411] data -> labels
I0110 22:50:10.191211 2090763008 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/train_hdf.txt
I0110 22:50:10.191239 2090763008 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I0110 22:50:10.197204 2090763008 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0110 22:50:10.716998 2090763008 net.cpp:150] Setting up data
I0110 22:50:10.717031 2090763008 net.cpp:157] Top shape: 450 3 125 125 (21093750)
I0110 22:50:10.717044 2090763008 net.cpp:157] Top shape: 450 4 (1800)
I0110 22:50:10.717051 2090763008 net.cpp:165] Memory required for data: 84382200
I0110 22:50:10.717059 2090763008 layer_factory.hpp:76] Creating layer conv1
I0110 22:50:10.717073 2090763008 net.cpp:106] Creating Layer conv1
I0110 22:50:10.717082 2090763008 net.cpp:454] conv1 <- data
I0110 22:50:10.717094 2090763008 net.cpp:411] conv1 -> conv1
I0110 22:50:10.723155 2090763008 net.cpp:150] Setting up conv1
I0110 22:50:10.723166 2090763008 net.cpp:157] Top shape: 450 16 115 115 (95220000)
I0110 22:50:10.723173 2090763008 net.cpp:165] Memory required for data: 465262200
I0110 22:50:10.723183 2090763008 layer_factory.hpp:76] Creating layer tanhConv1
I0110 22:50:10.723191 2090763008 net.cpp:106] Creating Layer tanhConv1
I0110 22:50:10.723196 2090763008 net.cpp:454] tanhConv1 <- conv1
I0110 22:50:10.723203 2090763008 net.cpp:397] tanhConv1 -> conv1 (in-place)
I0110 22:50:10.723212 2090763008 net.cpp:150] Setting up tanhConv1
I0110 22:50:10.723238 2090763008 net.cpp:157] Top shape: 450 16 115 115 (95220000)
I0110 22:50:10.723245 2090763008 net.cpp:165] Memory required for data: 846142200
I0110 22:50:10.723249 2090763008 layer_factory.hpp:76] Creating layer conv2
I0110 22:50:10.723258 2090763008 net.cpp:106] Creating Layer conv2
I0110 22:50:10.723261 2090763008 net.cpp:454] conv2 <- conv1
I0110 22:50:10.723268 2090763008 net.cpp:411] conv2 -> conv2
I0110 22:50:10.723613 2090763008 net.cpp:150] Setting up conv2
I0110 22:50:10.723619 2090763008 net.cpp:157] Top shape: 450 16 53 53 (20224800)
I0110 22:50:10.723625 2090763008 net.cpp:165] Memory required for data: 927041400
I0110 22:50:10.723633 2090763008 layer_factory.hpp:76] Creating layer tanhConv2
I0110 22:50:10.723640 2090763008 net.cpp:106] Creating Layer tanhConv2
I0110 22:50:10.723645 2090763008 net.cpp:454] tanhConv2 <- conv2
I0110 22:50:10.723650 2090763008 net.cpp:397] tanhConv2 -> conv2 (in-place)
I0110 22:50:10.723656 2090763008 net.cpp:150] Setting up tanhConv2
I0110 22:50:10.723660 2090763008 net.cpp:157] Top shape: 450 16 53 53 (20224800)
I0110 22:50:10.723665 2090763008 net.cpp:165] Memory required for data: 1007940600
I0110 22:50:10.723670 2090763008 layer_factory.hpp:76] Creating layer fc1
I0110 22:50:10.723676 2090763008 net.cpp:106] Creating Layer fc1
I0110 22:50:10.723681 2090763008 net.cpp:454] fc1 <- conv2
I0110 22:50:10.723707 2090763008 net.cpp:411] fc1 -> fc1
I0110 22:50:10.725106 2090763008 net.cpp:150] Setting up fc1
I0110 22:50:10.725111 2090763008 net.cpp:157] Top shape: 450 4 (1800)
I0110 22:50:10.725116 2090763008 net.cpp:165] Memory required for data: 1007947800
I0110 22:50:10.725124 2090763008 layer_factory.hpp:76] Creating layer out
I0110 22:50:10.725134 2090763008 net.cpp:106] Creating Layer out
I0110 22:50:10.725138 2090763008 net.cpp:454] out <- fc1
I0110 22:50:10.725143 2090763008 net.cpp:411] out -> out
I0110 22:50:10.725150 2090763008 net.cpp:150] Setting up out
I0110 22:50:10.725154 2090763008 net.cpp:157] Top shape: 450 4 (1800)
I0110 22:50:10.725158 2090763008 net.cpp:165] Memory required for data: 1007955000
I0110 22:50:10.725162 2090763008 layer_factory.hpp:76] Creating layer loss
I0110 22:50:10.725167 2090763008 net.cpp:106] Creating Layer loss
I0110 22:50:10.725172 2090763008 net.cpp:454] loss <- out
I0110 22:50:10.725175 2090763008 net.cpp:454] loss <- labels
I0110 22:50:10.725180 2090763008 net.cpp:411] loss -> loss
I0110 22:50:10.725190 2090763008 net.cpp:150] Setting up loss
I0110 22:50:10.725194 2090763008 net.cpp:157] Top shape: (1)
I0110 22:50:10.725198 2090763008 net.cpp:160]     with loss weight 1
I0110 22:50:10.725208 2090763008 net.cpp:165] Memory required for data: 1007955004
I0110 22:50:10.725213 2090763008 net.cpp:226] loss needs backward computation.
I0110 22:50:10.725216 2090763008 net.cpp:226] out needs backward computation.
I0110 22:50:10.725220 2090763008 net.cpp:226] fc1 needs backward computation.
I0110 22:50:10.725224 2090763008 net.cpp:226] tanhConv2 needs backward computation.
I0110 22:50:10.725227 2090763008 net.cpp:226] conv2 needs backward computation.
I0110 22:50:10.725231 2090763008 net.cpp:226] tanhConv1 needs backward computation.
I0110 22:50:10.725256 2090763008 net.cpp:226] conv1 needs backward computation.
I0110 22:50:10.725267 2090763008 net.cpp:228] data does not need backward computation.
I0110 22:50:10.725271 2090763008 net.cpp:270] This network produces output loss
I0110 22:50:10.725278 2090763008 net.cpp:283] Network initialization done.
I0110 22:50:10.725466 2090763008 solver.cpp:180] Creating test net (#0) specified by net file: /Users/JonathanLee/Desktop/sandbox/vision/Net/nets/net8/trainer8.prototxt
I0110 22:50:10.725487 2090763008 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0110 22:50:10.725497 2090763008 net.cpp:49] Initializing net from parameters: 
name: "net"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "/Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt"
    batch_size: 120
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanhConv1"
  type: "TanH"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 11
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "tanhConv2"
  type: "TanH"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "conv2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "out"
  type: "TanH"
  bottom: "fc1"
  top: "out"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "out"
  bottom: "labels"
  top: "loss"
}
I0110 22:50:10.725611 2090763008 layer_factory.hpp:76] Creating layer data
I0110 22:50:10.725620 2090763008 net.cpp:106] Creating Layer data
I0110 22:50:10.725625 2090763008 net.cpp:411] data -> data
I0110 22:50:10.725632 2090763008 net.cpp:411] data -> labels
I0110 22:50:10.725641 2090763008 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: /Users/JonathanLee/Desktop/sandbox/vision/Net/hdf/test_hdf.txt
I0110 22:50:10.725664 2090763008 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I0110 22:50:10.870393 2090763008 net.cpp:150] Setting up data
I0110 22:50:10.870416 2090763008 net.cpp:157] Top shape: 120 3 125 125 (5625000)
I0110 22:50:10.870425 2090763008 net.cpp:157] Top shape: 120 4 (480)
I0110 22:50:10.870430 2090763008 net.cpp:165] Memory required for data: 22501920
I0110 22:50:10.870437 2090763008 layer_factory.hpp:76] Creating layer conv1
I0110 22:50:10.870450 2090763008 net.cpp:106] Creating Layer conv1
I0110 22:50:10.870455 2090763008 net.cpp:454] conv1 <- data
I0110 22:50:10.870463 2090763008 net.cpp:411] conv1 -> conv1
I0110 22:50:10.870580 2090763008 net.cpp:150] Setting up conv1
I0110 22:50:10.870587 2090763008 net.cpp:157] Top shape: 120 16 115 115 (25392000)
I0110 22:50:10.870594 2090763008 net.cpp:165] Memory required for data: 124069920
I0110 22:50:10.870601 2090763008 layer_factory.hpp:76] Creating layer tanhConv1
I0110 22:50:10.870609 2090763008 net.cpp:106] Creating Layer tanhConv1
I0110 22:50:10.870614 2090763008 net.cpp:454] tanhConv1 <- conv1
I0110 22:50:10.870618 2090763008 net.cpp:397] tanhConv1 -> conv1 (in-place)
I0110 22:50:10.870627 2090763008 net.cpp:150] Setting up tanhConv1
I0110 22:50:10.870635 2090763008 net.cpp:157] Top shape: 120 16 115 115 (25392000)
I0110 22:50:10.870641 2090763008 net.cpp:165] Memory required for data: 225637920
I0110 22:50:10.870645 2090763008 layer_factory.hpp:76] Creating layer conv2
I0110 22:50:10.870652 2090763008 net.cpp:106] Creating Layer conv2
I0110 22:50:10.870657 2090763008 net.cpp:454] conv2 <- conv1
I0110 22:50:10.870666 2090763008 net.cpp:411] conv2 -> conv2
I0110 22:50:10.870981 2090763008 net.cpp:150] Setting up conv2
I0110 22:50:10.870987 2090763008 net.cpp:157] Top shape: 120 16 53 53 (5393280)
I0110 22:50:10.870992 2090763008 net.cpp:165] Memory required for data: 247211040
I0110 22:50:10.871000 2090763008 layer_factory.hpp:76] Creating layer tanhConv2
I0110 22:50:10.871006 2090763008 net.cpp:106] Creating Layer tanhConv2
I0110 22:50:10.871011 2090763008 net.cpp:454] tanhConv2 <- conv2
I0110 22:50:10.871017 2090763008 net.cpp:397] tanhConv2 -> conv2 (in-place)
I0110 22:50:10.871026 2090763008 net.cpp:150] Setting up tanhConv2
I0110 22:50:10.871032 2090763008 net.cpp:157] Top shape: 120 16 53 53 (5393280)
I0110 22:50:10.871088 2090763008 net.cpp:165] Memory required for data: 268784160
I0110 22:50:10.871099 2090763008 layer_factory.hpp:76] Creating layer fc1
I0110 22:50:10.871115 2090763008 net.cpp:106] Creating Layer fc1
I0110 22:50:10.871121 2090763008 net.cpp:454] fc1 <- conv2
I0110 22:50:10.871129 2090763008 net.cpp:411] fc1 -> fc1
I0110 22:50:10.872871 2090763008 net.cpp:150] Setting up fc1
I0110 22:50:10.872884 2090763008 net.cpp:157] Top shape: 120 4 (480)
I0110 22:50:10.872889 2090763008 net.cpp:165] Memory required for data: 268786080
I0110 22:50:10.872897 2090763008 layer_factory.hpp:76] Creating layer out
I0110 22:50:10.872903 2090763008 net.cpp:106] Creating Layer out
I0110 22:50:10.872907 2090763008 net.cpp:454] out <- fc1
I0110 22:50:10.872913 2090763008 net.cpp:411] out -> out
I0110 22:50:10.872920 2090763008 net.cpp:150] Setting up out
I0110 22:50:10.872925 2090763008 net.cpp:157] Top shape: 120 4 (480)
I0110 22:50:10.872928 2090763008 net.cpp:165] Memory required for data: 268788000
I0110 22:50:10.872932 2090763008 layer_factory.hpp:76] Creating layer loss
I0110 22:50:10.872937 2090763008 net.cpp:106] Creating Layer loss
I0110 22:50:10.872942 2090763008 net.cpp:454] loss <- out
I0110 22:50:10.872946 2090763008 net.cpp:454] loss <- labels
I0110 22:50:10.872953 2090763008 net.cpp:411] loss -> loss
I0110 22:50:10.872961 2090763008 net.cpp:150] Setting up loss
I0110 22:50:10.872964 2090763008 net.cpp:157] Top shape: (1)
I0110 22:50:10.872968 2090763008 net.cpp:160]     with loss weight 1
I0110 22:50:10.872974 2090763008 net.cpp:165] Memory required for data: 268788004
I0110 22:50:10.872978 2090763008 net.cpp:226] loss needs backward computation.
I0110 22:50:10.872982 2090763008 net.cpp:226] out needs backward computation.
I0110 22:50:10.872987 2090763008 net.cpp:226] fc1 needs backward computation.
I0110 22:50:10.872990 2090763008 net.cpp:226] tanhConv2 needs backward computation.
I0110 22:50:10.872994 2090763008 net.cpp:226] conv2 needs backward computation.
I0110 22:50:10.872998 2090763008 net.cpp:226] tanhConv1 needs backward computation.
I0110 22:50:10.873002 2090763008 net.cpp:226] conv1 needs backward computation.
I0110 22:50:10.873006 2090763008 net.cpp:228] data does not need backward computation.
I0110 22:50:10.873009 2090763008 net.cpp:270] This network produces output loss
I0110 22:50:10.873015 2090763008 net.cpp:283] Network initialization done.
I0110 22:50:10.873060 2090763008 solver.cpp:59] Solver scaffolding done.
I0110 22:50:10.873086 2090763008 caffe.cpp:212] Starting Optimization
I0110 22:50:10.873091 2090763008 solver.cpp:287] Solving net
I0110 22:50:10.873095 2090763008 solver.cpp:288] Learning Rate Policy: step
I0110 22:50:10.873546 2090763008 solver.cpp:340] Iteration 0, Testing net (#0)
I0110 22:50:12.962229 2090763008 solver.cpp:408]     Test net output #0: loss = 0.526006 (* 1 = 0.526006 loss)
I0110 22:50:33.356498 2090763008 solver.cpp:236] Iteration 0, loss = 0.589087
I0110 22:50:33.356525 2090763008 solver.cpp:252]     Train net output #0: loss = 0.589087 (* 1 = 0.589087 loss)
I0110 22:50:33.357297 2090763008 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0110 22:50:33.357841 2090763008 solver.cpp:340] Iteration 1, Testing net (#0)
I0110 22:50:35.381320 2090763008 solver.cpp:408]     Test net output #0: loss = 0.530366 (* 1 = 0.530366 loss)
I0110 22:50:56.381655 2090763008 solver.cpp:236] Iteration 1, loss = 0.5566
I0110 22:50:56.381695 2090763008 solver.cpp:252]     Train net output #0: loss = 0.5566 (* 1 = 0.5566 loss)
I0110 22:50:56.381702 2090763008 sgd_solver.cpp:106] Iteration 1, lr = 0.001
I0110 22:50:56.382125 2090763008 solver.cpp:340] Iteration 2, Testing net (#0)
I0110 22:50:58.456997 2090763008 solver.cpp:408]     Test net output #0: loss = 0.462783 (* 1 = 0.462783 loss)
